{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the same as the code from video 6, but only trained on unsong. The training data is the raw text of unsong, with most special characters removed. Everything written in lower case and each line as one individual, thing to be looked at. I also removed very short and long lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_line_len = 10\n",
    "max_line_len = 120\n",
    "\n",
    "block_size = 16\n",
    "# context length: how many characters do we take to predict the next one?\n",
    "\n",
    "n_embd = 16  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 128  # the number of neurons in the hidden layer of the MLP\n",
    "max_steps = 85000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27591\n",
      "210\n",
      "['', 'prologue', 'i', 'in retrospect, there had been omens and portents.', 'we are now approaching lunar sunrise, said william anders,', 'and for all the people back on earth, the crew of apollo  has a message', 'that we would like to send to you.', 'rivers flowed uphill. a new star was seen in the night sky. a']\n"
     ]
    }
   ],
   "source": [
    "# read in all the lines\n",
    "all_lines = open(\"clean_unsong.txt\", \"r\").read().splitlines()\n",
    "print(len(all_lines))\n",
    "print(max(len(line) for line in all_lines))\n",
    "print(all_lines[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for line in all_lines:\n",
    "    if min_line_len < len(line) < max_line_len:\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: ',', 3: '.', 4: '?', 5: 'a', 6: 'b', 7: 'c', 8: 'd', 9: 'e', 10: 'f', 11: 'g', 12: 'h', 13: 'i', 14: 'j', 15: 'k', 16: 'l', 17: 'm', 18: 'n', 19: 'o', 20: 'p', 21: 'q', 22: 'r', 23: 's', 24: 't', 25: 'u', 26: 'v', 27: 'w', 28: 'x', 29: 'y', 30: 'z', 0: '-'}\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(lines))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['-'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle up the words\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1017351, 16]) torch.Size([1017351])\n",
      "torch.Size([127001, 16]) torch.Size([127001])\n",
      "torch.Size([127411, 16]) torch.Size([127411])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "def build_dataset(lines):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in lines:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '-':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "n1 = int(0.8 * len(lines))\n",
    "n2 = int(0.9 * len(lines))\n",
    "Xtr, Ytr = build_dataset(lines[:n1])  # 80%\n",
    "Xdev, Ydev = build_dataset(lines[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(lines[n2:])  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- --> o\n",
      "---------------o --> f\n",
      "--------------of -->  \n",
      "-------------of  --> d\n",
      "------------of d --> a\n",
      "-----------of da --> y\n",
      "----------of day --> .\n",
      "---------of day. -->  \n",
      "--------of day.  --> a\n",
      "-------of day. a --> l\n",
      "------of day. al --> l\n",
      "-----of day. all -->  \n",
      "----of day. all  --> o\n",
      "---of day. all o --> f\n",
      "--of day. all of -->  \n",
      "-of day. all of  --> t\n",
      "of day. all of t --> h\n",
      "f day. all of th --> e\n",
      " day. all of the --> s\n",
      "day. all of thes --> e\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near copy paste of the layers we have developed in Part 3\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "\n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T//self.n, C*self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    # get parameters of all layers and stretch them out into one list\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42); # seed rng for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107919\n"
     ]
    }
   ],
   "source": [
    "# hierarchical network\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = model.parameters()\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/  85000: 3.4286\n",
      "  10000/  85000: 1.9855\n",
      "  20000/  85000: 1.6178\n",
      "  30000/  85000: 1.9301\n",
      "  40000/  85000: 1.8694\n",
      "  50000/  85000: 1.6738\n",
      "  60000/  85000: 1.6048\n",
      "  70000/  85000: 1.6871\n",
      "  80000/  85000: 1.5885\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update: simple SGD\n",
    "    lr = 0.1 if i < max_steps * 0.9 else 0.01  # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lossi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.plot(torch.tensor(\u001b[43mlossi\u001b[49m).view(-\u001b[32m1\u001b[39m, \u001b[32m1000\u001b[39m).mean(\u001b[32m1\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'lossi' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# put layers into eval mode (needed for batchnorm especially)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m.layers:\n\u001b[32m      3\u001b[39m     layer.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# put layers into eval mode (needed for batchnorm especially)\n",
    "for layer in model.layers:\n",
    "    layer.training = False\n",
    "\n",
    "\n",
    "# evaluate the loss\n",
    "@torch.no_grad() # this decorator disables gradient tracking inside pytorch\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  logits = model(x)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(f'{split}, {loss.item():.3f}')\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was the time water in made, aaron again deasing intenty, then a have one with powing sins treationah, we thuse the ground. i raw\n",
      "books otherstopprises expuforest.\n",
      "that the bized, and we care on the\n",
      "every ger name of the for a securd talkignaphisit for\n",
      "connectly. kingles. any\n",
      "he carridon. suffering boin gianty. i this which siep are was ight, and appeared nathank great he\n",
      "lower and they descrupse children. that he had an it. surving clouds when could an into importation, he moment. the commanded inside magnitured you cant to guys of crack not have\n",
      "too a piarnued. ran clarternous.\n",
      "califohs and sun a gldelf heavens in hetfferzor. im was scrusties, and got have some to\n",
      "this ockepresmeek to her faunding, but thinks too captain offs undering of not started intil follow, the first cansion hir just desthee made me books, said jefile that most nos ground, letters. that\n",
      "scause things with publication she sparkader, wheeld and pullen ahanial be celread\n",
      "paperty that almost exogents get table the lord ritual said, but tock. it was state\n",
      "on his with themselves of and her, there askin she himed lide demoneeded my fabns, they seem,\n",
      "he was a grantaged in the towering be wets universe who wanted at ministestinations human differing over in keep. cafely to can featured\n",
      "gaunto make years. theres\n",
      "whule else shabled the think with just machinery\n",
      "was hardyeon from morah\n",
      "god off argistand, i opent, a child very litake elished kind a just two like the cramc old some with shaper clash.\n",
      "infinally name. i begive all time in the leafn\n",
      "sire, and aland is\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...(zeros)\n",
    "    while True:\n",
    "        # forward pass the neural net\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        if ix != 0:\n",
    "            out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))  # decode and print the generated word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Hyper-Parameters\n",
    "\n",
    "| Run | Min line len | max line len |  block_size | n_embd | n_hidden | model       | lr  | train | val | params | max_steps | batch_size |\n",
    "| --- | ---          | ---          | ---         | ---    | ---      | ---         | --- | --- | --- | ---   | ---          | ---      |\n",
    "|  1  |  10          |  30          | 8           |   24   |  128     | e flbt*3 l | 0.1  | 1.794 |1.970 | 77191 | 5000 |    32|        |\n",
    "|  2  |              |      60        |             |        |          |         |       |         |   |        |    |      |        |\n",
    "|  3  |              |               |             |        |          |         |       |   1.707        | 1.795    |        | 25000   |      |        |\n",
    "|  4  |              |               |             |     16   |          |         |       |         | 1.660    |        | 25000   |      |        |\n",
    "\n",
    "\n",
    "## Outputs\n",
    "\n",
    "\n",
    "### Run 1\n",
    "\n",
    "\n",
    "shat as langelet\n",
    "yi , inkakiag.\n",
    "god to for thing morn?\n",
    "a nating bres?\n",
    "the krust rove.\n",
    "shupielpag\n",
    "pbooid, teelof ation.\n",
    "ditighous hne i lated oull.\n",
    "wor hdie to happere\n",
    "last, i nigld remettmtion.\n",
    "thatj issation.\n",
    "brobel you from whilovisate.\n",
    "nate to reelon.\n",
    "plought the?\n",
    "wastaticanly wamilve.\n",
    "it to hor lest destit dom.\n",
    "down turiestrone ming, thatlom onebous than\n",
    "a goal an?\n",
    "thy bour that sabla rabury.\n",
    "his could?\n",
    "\n",
    "![alt text](b7593670-30a3-4b1f-a3be-486470694d78.png)\n",
    "\n",
    "\n",
    "\n",
    "### Run 2\n",
    "\n",
    "\n",
    "of thaples freated by sucas sled at if ihselice yom.\n",
    "waic or to on ter crous she jusu anot to be. am dodk teireens. ksobk in well kstsice of\n",
    "hadan seo to khou\n",
    "greard tot oth atdelled why oullnt sowq is ject dadens. the. and the comen witels the cont it ean\n",
    "\n",
    "\n",
    "### Run 3\n",
    "\n",
    "whaters phen, said a for um, happenical one seeh stop ment. oke\n",
    "see sive me, screast\n",
    "comma tritinal war, so sone . . .\n",
    "then difta a mongs it,\n",
    "to she laged.\n",
    "on that. you will by mysis. i asket.\n",
    "well an it. bletea of habribe going mypaary build think was a aare in mooted awaysh i he some who do vows tnen presever her tolla comtally, as into thech will gmorize big i dxank i did coung kae in i with mose the lupdr asked parsing it is poesen us. starn, whose it feels speod, i seefern to got starthy. him all,.  him his ip men. lejtion spectliel. i tosly wure a my pupia starcians spaccifally spowred heave lace. im it it,\n",
    "anage of the maybe somextatifu. where balch ituelly frre fhen different\n",
    "a croul . . . stohne as terrible\n",
    "\n",
    "\n",
    "\n",
    "### Run 4\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
